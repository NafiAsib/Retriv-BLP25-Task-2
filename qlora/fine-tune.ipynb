{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # None for auto detection\n",
    "load_in_4bit = True  # Use 4bit quantization for QLoRA\n",
    "\n",
    "print(\"Loading Qwen2.5-Coder-14B-Instruct for fine-tuning...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89378d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # LoRA rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c0b5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c333496",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpp_df = pd.read_csv('PATH_TO_MBPP_CSV')\n",
    "\n",
    "print(f\"Dataset loaded: {len(mbpp_df)} samples\")\n",
    "print(f\"Columns: {list(mbpp_df.columns)}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(mbpp_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "\n",
    "def format_mbpp_sample(row):\n",
    "    \"\"\"Convert MBPP sample to conversational format\"\"\"\n",
    "    description = row['description']\n",
    "    code = row['code']\n",
    "    tests = row['tests']\n",
    "    \n",
    "    # Create comprehensive system message for code generation\n",
    "    system_message = \"\"\"You are an expert Python programmer specializing in algorithmic problem solving. Your task is to generate clean, efficient, and correct Python code that passes all given test cases.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Analyze the problem description carefully\n",
    "2. Study the test cases to understand input/output patterns and edge cases\n",
    "3. Write clean, readable Python code with proper error handling\n",
    "4. Ensure your solution passes ALL test cases exactly\n",
    "5. Use appropriate algorithms and data structures for efficiency\n",
    "6. Handle edge cases like empty inputs, None values, boundary conditions\n",
    "7. Follow Python best practices and coding standards\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Provide ONLY the Python code implementation\n",
    "- Do NOT include explanations, comments about the approach, or markdown formatting\n",
    "- Do NOT wrap code in backticks or code blocks\n",
    "- Write complete, executable functions that solve the problem\n",
    "\n",
    "Your code will be tested against the provided test cases, so accuracy is paramount.\"\"\"\n",
    "    \n",
    "    # Create user prompt with problem and test cases\n",
    "    user_prompt = f\"\"\"Problem: {description}\n",
    "\n",
    "Test Cases:\n",
    "{tests}\n",
    "\n",
    "Generate the Python code solution that passes all test cases.\"\"\"\n",
    "    \n",
    "    # Create conversation\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": code}\n",
    "    ]\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "print(\"Converting MBPP dataset to conversational format...\")\n",
    "conversations = []\n",
    "\n",
    "for _, row in tqdm(mbpp_df.iterrows(), total=len(mbpp_df), desc=\"Processing samples\"):\n",
    "    conversation = format_mbpp_sample(row)\n",
    "    conversations.append(conversation)\n",
    "\n",
    "dataset_dict = {\"conversations\": conversations}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "print(f\"Dataset converted: {len(dataset)} conversations ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ab5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "print(\"Chat templates applied to dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af784e3b",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5681380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Better for code generation tasks\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 3,  # 2 epochs as requested\n",
    "        learning_rate = 2e-4,  # 2e-4 as requested  \n",
    "        logging_steps = 50,    # 50 steps for logging as requested\n",
    "        optim = \"paged_adamw_8bit\", # Memory efficient\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 2,\n",
    "        load_best_model_at_end = False,\n",
    "        eval_strategy = \"no\",  # No validation set for now\n",
    "        dataloader_pin_memory = False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training on responses only (ignore system/user parts in loss)\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "print(\"Configured to train only on assistant responses (code generation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590645b2",
   "metadata": {},
   "source": [
    "## Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Memory reserved before training: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(f\"Training time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"Peak memory: {used_memory} GB ({used_percentage}% of total)\")\n",
    "print(f\"Memory for training: {used_memory_for_lora} GB ({lora_percentage}% of total)\")\n",
    "print(f\"Final training loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
    "print(f\"Training samples processed: {trainer_stats.metrics.get('train_samples', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ac2fe",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc01082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"qwen25_mbpp_finetuned\"\n",
    "\n",
    "print(f\"Saving fine-tuned model to: {model_save_path}\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Saved files in: {model_save_path}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
