{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e716f031",
   "metadata": {},
   "source": [
    "## Enhanced Inference with Fine-tuned Qwen2.5-Coder\n",
    "\n",
    "This notebook performs inference using the fine-tuned Qwen2.5-Coder model on the test dataset with advanced test-driven refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ed3a3",
   "metadata": {},
   "source": [
    "## Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67949902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import signal\n",
    "import sys\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management and system check\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# System resources\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected - using CPU (will be very slow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409a204",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_data_path = 'PATH_TO_TRANSLATED_DATESET'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "print(f\"Loaded {len(test_df)} test samples\")\n",
    "print(f\"Sample data structure: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned Qwen2.5-Coder model...\")\n",
    "\n",
    "# First try to load the fine-tuned model, fallback to base model if not available\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Try loading fine-tuned model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"qwen25_mbpp_finetuned\",  # Fine-tuned model path\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"Fine-tuned model loaded successfully!\")\n",
    "    \n",
    "except:\n",
    "    print(\"⚠️  Fine-tuned model not found, loading base model...\")\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=\"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    model = None\n",
    "    tokenizer = pipe.tokenizer\n",
    "\n",
    "# Create pipeline function for unified interface\n",
    "if model is not None:\n",
    "    # Fine-tuned model inference\n",
    "    def generate_code(prompt, **kwargs):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            prompt if isinstance(prompt, list) else [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=kwargs.get('max_new_tokens', 768),\n",
    "            temperature=kwargs.get('temperature', 0.1),\n",
    "            top_p=kwargs.get('top_p', 0.95),\n",
    "            do_sample=kwargs.get('do_sample', True),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "        return [{'generated_text': response}]\n",
    "        \n",
    "    pipe = type('Pipeline', (), {'tokenizer': tokenizer, '__call__': lambda self, prompt, **kwargs: generate_code(prompt, **kwargs)})()\n",
    "    print(\"Using fine-tuned model for generation\")\n",
    "else:\n",
    "    print(\"Using base model pipeline for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01180d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Format a single example into the required prompt format for fine-tuned model\"\"\"\n",
    "    instruction = example['instruction']\n",
    "    test_list = example['test_list']\n",
    "    \n",
    "    # Parse function name from instruction\n",
    "    function_name = \"unknown_function\"\n",
    "    if \"Example:\" in instruction:\n",
    "        example_part = instruction.split(\"Example:\")[1].strip()\n",
    "        func_match = re.search(r'(\\w+)\\s*\\(', example_part)\n",
    "        if func_match:\n",
    "            function_name = func_match.group(1)\n",
    "    \n",
    "    # Enhanced system message with algorithm-specific guidance\n",
    "    system_message = \"\"\"You are an expert Python programmer specializing in algorithmic problem solving. Your task is to generate clean, efficient, and correct Python code that passes all given test cases.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Analyze the problem description carefully and identify algorithm type (DP, graph, string, math, etc.)\n",
    "2. Study the test cases to understand input/output patterns, data types, and edge cases\n",
    "3. Write clean, readable Python code with proper error handling\n",
    "4. Ensure your solution passes ALL test cases exactly\n",
    "5. Use appropriate algorithms and data structures for efficiency\n",
    "6. Handle edge cases like empty inputs, None values, boundary conditions, negative numbers\n",
    "7. Follow Python best practices and coding standards\n",
    "8. Import necessary modules at the beginning if needed\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Provide ONLY the Python code implementation\n",
    "- Do NOT include explanations, comments about the approach, or markdown formatting\n",
    "- Do NOT wrap code in backticks or code blocks\n",
    "- Write complete, executable functions that solve the problem\n",
    "- Start with imports if needed (math, re, collections, itertools, etc.)\n",
    "\n",
    "ALGORITHM-SPECIFIC PATTERNS:\n",
    "\n",
    "Dynamic Programming:\n",
    "def dp_problem(n):\n",
    "    dp = [0] * (n + 1)\n",
    "    dp[0] = base_case\n",
    "    for i in range(1, n + 1):\n",
    "        dp[i] = recurrence_relation\n",
    "    return dp[n]\n",
    "\n",
    "String Processing:\n",
    "def string_problem(s):\n",
    "    if not s:  # Handle empty string\n",
    "        return default_value\n",
    "    # Process string with appropriate method\n",
    "    return result\n",
    "\n",
    "Mathematical:\n",
    "import math\n",
    "def math_problem(n):\n",
    "    if n <= 0:  # Handle edge cases\n",
    "        return base_case\n",
    "    # Use math functions efficiently\n",
    "    return result\n",
    "\n",
    "EXAMPLES OF CORRECT RESPONSES:\n",
    "\n",
    "Problem: Write a function to find the first repeated character in a string.\n",
    "Test Cases: assert first_repeated_char(\"abcabc\") == \"a\"\n",
    "\n",
    "Correct Response:\n",
    "def first_repeated_char(s):\n",
    "    seen = set()\n",
    "    for char in s:\n",
    "        if char in seen:\n",
    "            return char\n",
    "        seen.add(char)\n",
    "    return \"None\"\n",
    "\n",
    "Problem: Write a function to check if a number is prime.\n",
    "Test Cases: assert prime_num(13) == True\n",
    "\n",
    "Correct Response:\n",
    "import math\n",
    "def prime_num(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "Problem: Write a function to find similar elements in two tuples.\n",
    "Test Cases: assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\n",
    "\n",
    "Correct Response:\n",
    "def similar_elements(test_tup1, test_tup2):\n",
    "    result = tuple(sorted(set(test_tup1) & set(test_tup2)))\n",
    "    return result\n",
    "\n",
    "Problem: Write a function for dynamic programming - Fibonacci.\n",
    "Test Cases: assert fib(10) == 55\n",
    "\n",
    "Correct Response:\n",
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    dp = [0] * (n + 1)\n",
    "    dp[1] = 1\n",
    "    for i in range(2, n + 1):\n",
    "        dp[i] = dp[i-1] + dp[i-2]\n",
    "    return dp[n]\"\"\"\n",
    "        \n",
    "    user_prompt = f\"\"\"Problem: {instruction}\n",
    "\n",
    "Test Cases:\n",
    "{test_list}\n",
    "\n",
    "Expected Function Name: {function_name}\n",
    "\n",
    "ANALYSIS CHECKLIST:\n",
    "✓ Identify algorithm type (DP, graph, string manipulation, math, etc.)\n",
    "✓ Check input/output data types from test cases\n",
    "✓ Consider edge cases (empty, single element, boundary values)\n",
    "✓ Choose optimal data structures and algorithms\n",
    "✓ Ensure exact return format matches test expectations\n",
    "\n",
    "Generate the Python code solution that passes ALL test cases.\"\"\"\n",
    "        \n",
    "    # Format for chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    # Apply chat template\n",
    "    formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for inference\n",
    "instructions_data = []\n",
    "formatted_prompts = []\n",
    "ids_list = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), desc=\"Preparing data\", unit=\"row\", total=len(test_df)):\n",
    "    instructions_data.append({\n",
    "        'instruction': row['instruction'],\n",
    "        'test_list': row['test_list'],\n",
    "        'id': row['id']\n",
    "    })\n",
    "\n",
    "for item in tqdm(instructions_data, desc=\"Formatting prompts\", unit=\"prompt\"):\n",
    "    formatted_prompt = format_prompt(item)\n",
    "    formatted_prompts.append(formatted_prompt)\n",
    "    ids_list.append(item['id'])\n",
    "\n",
    "print(f\"Formatted {len(formatted_prompts)} prompts\")\n",
    "\n",
    "# Create dataset\n",
    "dataset_dict = {\n",
    "    'prompt': formatted_prompts,\n",
    "    'id': ids_list\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b531e8d",
   "metadata": {},
   "source": [
    "## Test Execution Engine\n",
    "Advanced testing system that executes generated code against test cases with comprehensive error reporting and timeout protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout_handler(signum, frame):\n",
    "    \"\"\"Handler for timeout signals\"\"\"\n",
    "    raise TimeoutError(\"Test execution timed out\")\n",
    "\n",
    "def execute_and_test_code(code, test_cases_str, timeout=15):\n",
    "    \"\"\"\n",
    "    Execute code and run test cases with detailed error reporting\n",
    "    Returns: (success, summary, detailed_results)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean code - NO MARKDOWN WRAPPING REMOVAL since we don't generate it\n",
    "    clean_code = code.strip()\n",
    "    \n",
    "    # Parse test cases\n",
    "    try:\n",
    "        test_cases = ast.literal_eval(test_cases_str)\n",
    "    except Exception as e:\n",
    "        return False, f\"Test parsing error: {e}\", []\n",
    "    \n",
    "    # Execute the function definition\n",
    "    namespace = {}\n",
    "    try:\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(timeout)\n",
    "        exec(clean_code, namespace)\n",
    "        signal.alarm(0)\n",
    "    except TimeoutError:\n",
    "        print(f\"Code execution timeout ({timeout}s)\")\n",
    "        return False, f\"Code execution timeout ({timeout}s)\", []\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Syntax error: {e}\", []\n",
    "    except Exception as e:\n",
    "        return False, f\"Runtime error in function definition: {e}\", []\n",
    "    \n",
    "    # Run each test case\n",
    "    results = []\n",
    "    passed_count = 0\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        try:\n",
    "            signal.alarm(timeout)\n",
    "            \n",
    "            # Execute test case\n",
    "            exec(test_case, namespace)\n",
    "            \n",
    "            signal.alarm(0)\n",
    "            results.append({\n",
    "                'test_case': test_case,\n",
    "                'status': 'PASSED',\n",
    "                'error': None,\n",
    "                'index': i + 1\n",
    "            })\n",
    "            passed_count += 1\n",
    "            \n",
    "        except TimeoutError:\n",
    "            print(f\"Test case {i+1} timeout ({timeout}s)\")\n",
    "            results.append({\n",
    "                'test_case': test_case,\n",
    "                'status': 'TIMEOUT',\n",
    "                'error': f'Test execution timeout ({timeout}s)',\n",
    "                'index': i + 1\n",
    "            })\n",
    "            break  # Stop on timeout\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            results.append({\n",
    "                'test_case': test_case,\n",
    "                'status': 'ASSERTION_FAILED',\n",
    "                'error': str(e) if str(e) else 'Assertion failed - expected vs actual values differ',\n",
    "                'index': i + 1\n",
    "            })\n",
    "            break  # Stop on first failure\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'test_case': test_case,\n",
    "                'status': 'RUNTIME_ERROR',\n",
    "                'error': str(e),\n",
    "                'index': i + 1\n",
    "            })\n",
    "            break  # Stop on first error\n",
    "        finally:\n",
    "            signal.alarm(0)\n",
    "    \n",
    "    success = passed_count == len(test_cases)\n",
    "    summary = f\"Passed {passed_count}/{len(test_cases)} tests\"\n",
    "    \n",
    "    return success, summary, results\n",
    "\n",
    "def create_enhanced_error_feedback(test_results, instruction, test_cases_str, previous_attempts):\n",
    "    \"\"\"Create enhanced feedback for failed tests with pattern analysis\"\"\"\n",
    "    \n",
    "    if not test_results:\n",
    "        return \"\\nNo test results available. Please check if the function was defined correctly.\"\n",
    "    \n",
    "    # Find the first failed test\n",
    "    failed_test = next((r for r in test_results if r['status'] != 'PASSED'), None)\n",
    "    \n",
    "    if not failed_test:\n",
    "        return \"\\nAll tests passed!\"\n",
    "    \n",
    "    # Analyze previous attempts for patterns\n",
    "    attempt_analysis = \"\"\n",
    "    if len(previous_attempts) > 1:\n",
    "        attempt_analysis = f\"\"\"\n",
    "\n",
    "# PATTERN ANALYSIS FROM {len(previous_attempts)} ATTEMPTS:\n",
    "- Attempt 1: {len(previous_attempts[0])} characters\n",
    "- Latest: {len(previous_attempts[-1])} characters\n",
    "- Different approaches tried: {len(set(attempt[:50] for attempt in previous_attempts))}\n",
    "\n",
    "# AVOID REPEATING: The same logic pattern has failed multiple times. Try a fundamentally different approach.\"\"\"\n",
    "\n",
    "    # Enhanced error analysis based on error type\n",
    "    specific_guidance = \"\"\n",
    "    if failed_test['status'] == 'ASSERTION_FAILED':\n",
    "        specific_guidance = \"\"\"\n",
    "## ASSERTION FAILURE GUIDANCE:\n",
    "- Check return data type (int, str, list, tuple, bool)\n",
    "- Verify exact return format matches expected output\n",
    "- Consider sorting if order doesn't matter\n",
    "- Handle empty cases explicitly\"\"\"\n",
    "    elif failed_test['status'] == 'RUNTIME_ERROR':\n",
    "        error_msg = failed_test['error'].lower()\n",
    "        if 'index' in error_msg or 'list' in error_msg:\n",
    "            specific_guidance = \"\"\"\n",
    "## INDEX/LIST ERROR GUIDANCE:\n",
    "- Check for empty list/string handling\n",
    "- Verify array bounds (0 to len-1)\n",
    "- Handle edge case when input is empty\"\"\"\n",
    "        elif 'key' in error_msg or 'dict' in error_msg:\n",
    "            specific_guidance = \"\"\"\n",
    "## DICTIONARY ERROR GUIDANCE:\n",
    "- Check if key exists before accessing\n",
    "- Use .get() method with default values\n",
    "- Initialize dictionaries properly\"\"\"\n",
    "        elif 'attribute' in error_msg:\n",
    "            specific_guidance = \"\"\"\n",
    "## ATTRIBUTE ERROR GUIDANCE:\n",
    "- Check object types before method calls\n",
    "- Verify variable is initialized\n",
    "- Import necessary modules\"\"\"\n",
    "    \n",
    "    # Create comprehensive error feedback\n",
    "    feedback = f\"\"\"\n",
    "\n",
    "## PREVIOUS ATTEMPT FAILED - ADVANCED DEBUGGING:\n",
    "\n",
    "- Error Type: {failed_test['status']}\n",
    "- Error Message: {failed_test['error']}\n",
    "- Failing Test Case: {failed_test['test_case']}\n",
    "- Failed at Test #{failed_test['index']} out of {len([r for r in test_results if 'index' in r])}\n",
    "\n",
    "{specific_guidance}\n",
    "\n",
    "{attempt_analysis}\n",
    "\n",
    "🔧 SYSTEMATIC DEBUGGING APPROACH:\n",
    "1. ANALYZE INPUT/OUTPUT: What data types and patterns do test cases show?\n",
    "2. EDGE CASE CHECK: Empty inputs, single elements, boundary values\n",
    "3. ALGORITHM CHOICE: Is this DP, greedy, two-pointer, sliding window, etc.?\n",
    "4. IMPLEMENTATION: Step through the failing test case manually\n",
    "5. IMPORTS: Add math, re, collections, itertools if needed\n",
    "\n",
    "# Original Task: {instruction}\n",
    "\n",
    "# CRITICAL SUCCESS FACTORS:\n",
    "- Function signature must match test case exactly\n",
    "- Return type must match expected output precisely  \n",
    "- Handle ALL edge cases shown in test patterns\n",
    "- Use efficient algorithm for the problem type\n",
    "\n",
    "## GENERATE A COMPLETELY NEW APPROACH - Previous attempts failed for a reason.\"\"\"\n",
    "\n",
    "    return feedback\n",
    "\n",
    "def create_error_feedback(test_results, instruction, test_cases_str):\n",
    "    \"\"\"Legacy function - kept for compatibility\"\"\"\n",
    "    return create_enhanced_error_feedback(test_results, instruction, test_cases_str, [])\n",
    "\n",
    "def get_function_name_from_test(test_cases_str):\n",
    "    \"\"\"Extract function name from test cases for better error reporting\"\"\"\n",
    "    try:\n",
    "        test_cases = ast.literal_eval(test_cases_str)\n",
    "        \n",
    "        if test_cases:\n",
    "            # Find function name in first test case\n",
    "            func_match = re.search(r'assert\\s+(\\w+)\\s*\\(', test_cases[0])\n",
    "            if func_match:\n",
    "                return func_match.group(1)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return \"function\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ac410",
   "metadata": {},
   "source": [
    "## Enhanced Generation with Test-Driven Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_testing(prompt, test_cases_str, instruction, sample_id, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Generate code with iterative testing and refinement using fine-tuned model\n",
    "    Returns: (generated_code, full_success, attempts_used, best_score, input_tokens, output_tokens)\n",
    "    \"\"\"\n",
    "    \n",
    "    best_code = None\n",
    "    best_score = 0\n",
    "    all_attempts = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    \n",
    "    func_name = get_function_name_from_test(test_cases_str)\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        temp = [0.1, 0.3, 0.5][attempt]\n",
    "        \n",
    "        try:\n",
    "            result = pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=768,  # Increased for complex problems\n",
    "                temperature=temp,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                return_full_text=False,\n",
    "                pad_token_id=pipe.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_code = result[0]['generated_text'].strip()\n",
    "            all_attempts.append(generated_code)\n",
    "            \n",
    "            success, summary, test_results = execute_and_test_code(\n",
    "                generated_code, test_cases_str\n",
    "            )\n",
    "            \n",
    "            if test_results:\n",
    "                passed_tests = sum(1 for r in test_results if r['status'] == 'PASSED')\n",
    "                total_tests = len([r for r in test_results if 'index' in r])\n",
    "                current_score = passed_tests / total_tests if total_tests > 0 else 0\n",
    "            else:\n",
    "                current_score = 0\n",
    "            \n",
    "            # Keep track of best attempt\n",
    "            if current_score > best_score or (current_score == best_score and best_code is None):\n",
    "                best_code = generated_code\n",
    "                best_score = current_score\n",
    "            \n",
    "            if success:\n",
    "                print(f\"ID {sample_id} - Attempt {attempt + 1}: {func_name}() - All tests passed! 🎉\")\n",
    "                return generated_code, True, attempt + 1, 1.0, total_input_tokens, total_output_tokens\n",
    "            \n",
    "            else:\n",
    "                print(f\"ID {sample_id} - Attempt {attempt + 1}: {func_name}() - {summary} (temp={temp})\")\n",
    "                \n",
    "                # Enhanced error feedback for next attempt (if not last attempt)\n",
    "                if attempt < max_attempts - 1:\n",
    "                    error_feedback = create_enhanced_error_feedback(test_results, instruction, test_cases_str, all_attempts)\n",
    "                    \n",
    "                    # Enhanced system content for retry\n",
    "                    system_content = \"\"\"You are an expert Python programmer specializing in algorithmic problem solving. Your task is to generate clean, efficient, and correct Python code that passes all given test cases.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Analyze the problem description carefully and identify the algorithm type (DP, graph, string manipulation, math, etc.)\n",
    "2. Study the test cases to understand input/output patterns, data types, and edge cases\n",
    "3. Write clean, readable Python code with proper error handling\n",
    "4. Ensure your solution passes ALL test cases exactly\n",
    "5. Use appropriate algorithms and data structures for efficiency\n",
    "6. Handle edge cases like empty inputs, None values, boundary conditions, negative numbers\n",
    "7. Follow Python best practices and coding standards\n",
    "8. Import necessary modules (math, re, collections, itertools, etc.) at the beginning\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Provide ONLY the Python code implementation\n",
    "- Do NOT include explanations, comments about the approach, or markdown formatting\n",
    "- Do NOT wrap code in backticks or code blocks\n",
    "- Write complete, executable functions that solve the problem\n",
    "- Start with necessary imports if needed\n",
    "\n",
    "ALGORITHM-SPECIFIC GUIDANCE:\n",
    "- Dynamic Programming: Use memoization or tabulation appropriately\n",
    "- String Processing: Consider edge cases with empty strings, special characters\n",
    "- Mathematical: Handle negative numbers, zero, overflow conditions\n",
    "- Data Structures: Choose optimal structures (set, dict, list, deque, etc.)\n",
    "- Graph Problems: Consider connectivity, cycles, traversal methods\n",
    "\n",
    "Your code will be tested against the provided test cases, so accuracy is paramount.\"\"\"\n",
    "\n",
    "                    # Enhanced user prompt with error feedback and previous attempts analysis\n",
    "                    enhanced_user_prompt = f\"\"\"Problem: {instruction}\n",
    "\n",
    "Test Cases:\n",
    "{test_cases_str}\n",
    "\n",
    "{error_feedback}\n",
    "\n",
    "PREVIOUS ATTEMPTS ANALYSIS:\n",
    "{len(all_attempts)} attempts made. Learn from these patterns to avoid repeating mistakes.\n",
    "\n",
    "Generate the Python code solution that passes ALL test cases.\"\"\"\n",
    "\n",
    "                    # Create retry prompt\n",
    "                    retry_messages = [\n",
    "                        {\"role\": \"system\", \"content\": system_content},\n",
    "                        {\"role\": \"user\", \"content\": enhanced_user_prompt}\n",
    "                    ]\n",
    "                    \n",
    "                    prompt = pipe.tokenizer.apply_chat_template(\n",
    "                        retry_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ID {sample_id} - Attempt {attempt + 1} generation failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Return best attempt\n",
    "    print(f\"ID {sample_id}: Used best attempt - {func_name}() with {best_score:.1%} pass rate\")\n",
    "    \n",
    "    return best_code or \"def placeholder(): pass\", False, max_attempts, best_score, total_input_tokens, total_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e426f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "failed_ids = []\n",
    "\n",
    "success_stats = {\n",
    "    'full_success': 0,       # 100% pass rate\n",
    "    'partial_success': 0,    # Some tests passed but not all\n",
    "    'complete_failure': 0,   # No tests passed or execution failed\n",
    "    'total_attempts': 0,     # Total generation attempts across all samples\n",
    "    'total_samples': 0,\n",
    "    'attempt_distribution': [0, 0, 0]  # Track success by attempt number\n",
    "}\n",
    "\n",
    "print(\"Starting ENHANCED Test-Driven Code Generation with Fine-tuned Model...\")\n",
    "\n",
    "for idx in tqdm(range(len(dataset)), desc=\"Enhanced generation\"):\n",
    "    try:\n",
    "        # Get sample data\n",
    "        prompt = dataset[idx]['prompt']\n",
    "        sample_id = dataset[idx]['id']\n",
    "        \n",
    "        # Get corresponding test cases and instruction from original data\n",
    "        original_row = test_df[test_df['id'] == sample_id].iloc[0]\n",
    "        test_cases_str = original_row['test_list']\n",
    "        instruction = original_row['instruction']\n",
    "        \n",
    "        # Generate with enhanced iterative testing and refinement\n",
    "        generated_code, full_success, attempts_used, final_score, input_tokens, output_tokens = generate_with_testing(\n",
    "            prompt, test_cases_str, instruction, sample_id, max_attempts=3\n",
    "        )\n",
    "        \n",
    "        responses.append(generated_code)\n",
    "        \n",
    "        # Update enhanced statistics\n",
    "        success_stats['total_attempts'] += attempts_used\n",
    "        success_stats['total_samples'] += 1\n",
    "        success_stats['total_input_tokens'] += input_tokens\n",
    "        success_stats['total_output_tokens'] += output_tokens\n",
    "        \n",
    "        if full_success:\n",
    "            success_stats['full_success'] += 1\n",
    "            # Track which attempt succeeded\n",
    "            if attempts_used <= 5:\n",
    "                success_stats['attempt_distribution'][attempts_used - 1] += 1\n",
    "        elif final_score > 0:\n",
    "            success_stats['partial_success'] += 1  \n",
    "        else:\n",
    "            success_stats['complete_failure'] += 1\n",
    "            \n",
    "        # Memory management - clear every 15 samples (more frequent)\n",
    "        if (idx + 1) % 15 == 0:\n",
    "            clear_memory()\n",
    "            print(f\"\\nMemory cleared after {idx + 1} samples\")\n",
    "            \n",
    "        # Enhanced progress update every 50 samples\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            current_success_rate = success_stats['full_success'] / success_stats['total_samples']\n",
    "            avg_attempts = success_stats['total_attempts'] / success_stats['total_samples']\n",
    "            avg_input_tokens = success_stats['total_input_tokens'] / success_stats['total_samples']\n",
    "            avg_output_tokens = success_stats['total_output_tokens'] / success_stats['total_samples']\n",
    "            \n",
    "            print(f\"\\nProgress Update after {idx + 1} samples:\")\n",
    "            print(f\"Success Rate: {current_success_rate:.1%} ({success_stats['full_success']}/{success_stats['total_samples']})\")\n",
    "            print(f\"Avg Attempts: {avg_attempts:.1f}\")\n",
    "            print(f\"Avg Input Tokens: {avg_input_tokens:.0f}\")\n",
    "            print(f\"Avg Output Tokens: {avg_output_tokens:.0f}\")\n",
    "            print(f\"Expected Final: {current_success_rate:.1%}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Complete failure for ID {sample_id}: {e}\")\n",
    "        failed_ids.append(sample_id)\n",
    "        responses.append(\"def placeholder(): pass\")\n",
    "        success_stats['complete_failure'] += 1\n",
    "        success_stats['total_samples'] += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "print(\"ENHANCED GENERATION WITH FINE-TUNED MODEL COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"raw_responses_backup.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)\n",
    "print(\"Successfully saved raw responses to 'raw_responses_backup.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json(\"test_df.json\", orient=\"records\", indent=2)\n",
    "print(\"Saved test_df to 'test_df.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
