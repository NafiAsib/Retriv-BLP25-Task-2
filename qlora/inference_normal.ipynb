{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67949902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import signal\n",
    "import sys\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected - using CPU (will be very slow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409a204",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_path = 'PATH_TO_TRANSLATED_DATESET'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "print(f\"Loaded {len(test_df)} test samples\")\n",
    "print(f\"Sample data structure: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fine-tuned Qwen2.5-Coder model...\")\n",
    "\n",
    "# First try to load the fine-tuned model, fallback to base model if not available\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Try loading fine-tuned model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"qwen25_mbpp_finetuned\",  # Fine-tuned model path\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"Fine-tuned model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fine-tuned model not found ({e}), loading base model...\")\n",
    "    # Use more conservative settings to avoid Triton issues\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=\"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        model_kwargs={\n",
    "            \"attn_implementation\": \"eager\",  # Avoid flash attention issues\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "    )\n",
    "    model = None\n",
    "    tokenizer = pipe.tokenizer\n",
    "\n",
    "# Create pipeline function for unified interface\n",
    "if model is not None:\n",
    "    # Fine-tuned model inference with proper device handling\n",
    "    def generate_code_ft(prompt, **kwargs):\n",
    "        try:\n",
    "            # Ensure we're working with the right device\n",
    "            device = next(model.parameters()).device\n",
    "            \n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                prompt if isinstance(prompt, list) else [{\"role\": \"user\", \"content\": prompt}],\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():  # Prevent gradient computation issues\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=kwargs.get('max_new_tokens', 768),\n",
    "                    temperature=kwargs.get('temperature', 0.1),\n",
    "                    top_p=kwargs.get('top_p', 0.95),\n",
    "                    do_sample=kwargs.get('do_sample', True),\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "            return [{'generated_text': response}]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fine-tuned model generation error: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    pipe = type('Pipeline', (), {'tokenizer': tokenizer, '__call__': lambda self, prompt, **kwargs: generate_code_ft(prompt, **kwargs)})()\n",
    "    print(\"Using fine-tuned model for generation\")\n",
    "else:\n",
    "    print(\"Using base model pipeline for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01180d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Format a single example into the required prompt format for fine-tuned model\"\"\"\n",
    "    instruction = example['instruction']\n",
    "    test_list = example['test_list']\n",
    "    \n",
    "    # Parse function name from instruction\n",
    "    function_name = \"unknown_function\"\n",
    "    if \"Example:\" in instruction:\n",
    "        example_part = instruction.split(\"Example:\")[1].strip()\n",
    "        func_match = re.search(r'(\\w+)\\s*\\(', example_part)\n",
    "        if func_match:\n",
    "            function_name = func_match.group(1)\n",
    "    \n",
    "    # Enhanced system message with algorithm-specific guidance\n",
    "    system_message = \"\"\"You are an expert Python programmer specializing in algorithmic problem solving. Your task is to generate clean, efficient, and correct Python code that passes all given test cases.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Analyze the problem description carefully and identify algorithm type (DP, graph, string, math, etc.)\n",
    "2. Study the test cases to understand input/output patterns, data types, and edge cases\n",
    "3. Write clean, readable Python code with proper error handling\n",
    "4. Ensure your solution passes ALL test cases exactly\n",
    "5. Use appropriate algorithms and data structures for efficiency\n",
    "6. Handle edge cases like empty inputs, None values, boundary conditions, negative numbers\n",
    "7. Follow Python best practices and coding standards\n",
    "8. Import necessary modules at the beginning if needed\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Provide ONLY the Python code implementation\n",
    "- Do NOT include explanations, comments about the approach, or markdown formatting\n",
    "- Do NOT wrap code in backticks or code blocks\n",
    "- Write complete, executable functions that solve the problem\n",
    "- Start with imports if needed (math, re, collections, itertools, etc.)\n",
    "\n",
    "ALGORITHM-SPECIFIC PATTERNS:\n",
    "\n",
    "Dynamic Programming:\n",
    "def dp_problem(n):\n",
    "    dp = [0] * (n + 1)\n",
    "    dp[0] = base_case\n",
    "    for i in range(1, n + 1):\n",
    "        dp[i] = recurrence_relation\n",
    "    return dp[n]\n",
    "\n",
    "String Processing:\n",
    "def string_problem(s):\n",
    "    if not s:  # Handle empty string\n",
    "        return default_value\n",
    "    # Process string with appropriate method\n",
    "    return result\n",
    "\n",
    "Mathematical:\n",
    "import math\n",
    "def math_problem(n):\n",
    "    if n <= 0:  # Handle edge cases\n",
    "        return base_case\n",
    "    # Use math functions efficiently\n",
    "    return result\n",
    "\n",
    "EXAMPLES OF CORRECT RESPONSES:\n",
    "\n",
    "Problem: Write a function to find the first repeated character in a string.\n",
    "Test Cases: assert first_repeated_char(\"abcabc\") == \"a\"\n",
    "\n",
    "Correct Response:\n",
    "def first_repeated_char(s):\n",
    "    seen = set()\n",
    "    for char in s:\n",
    "        if char in seen:\n",
    "            return char\n",
    "        seen.add(char)\n",
    "    return \"None\"\n",
    "\n",
    "Problem: Write a function to check if a number is prime.\n",
    "Test Cases: assert prime_num(13) == True\n",
    "\n",
    "Correct Response:\n",
    "import math\n",
    "def prime_num(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "Problem: Write a function to find similar elements in two tuples.\n",
    "Test Cases: assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\n",
    "\n",
    "Correct Response:\n",
    "def similar_elements(test_tup1, test_tup2):\n",
    "    result = tuple(sorted(set(test_tup1) & set(test_tup2)))\n",
    "    return result\n",
    "\n",
    "Problem: Write a function for dynamic programming - Fibonacci.\n",
    "Test Cases: assert fib(10) == 55\n",
    "\n",
    "Correct Response:\n",
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    dp = [0] * (n + 1)\n",
    "    dp[1] = 1\n",
    "    for i in range(2, n + 1):\n",
    "        dp[i] = dp[i-1] + dp[i-2]\n",
    "    return dp[n]\"\"\"\n",
    "        \n",
    "    user_prompt = f\"\"\"Problem: {instruction}\n",
    "\n",
    "Test Cases:\n",
    "{test_list}\n",
    "\n",
    "Expected Function Name: {function_name}\n",
    "\n",
    "ANALYSIS CHECKLIST:\n",
    "✓ Identify algorithm type (DP, graph, string manipulation, math, etc.)\n",
    "✓ Check input/output data types from test cases\n",
    "✓ Consider edge cases (empty, single element, boundary values)\n",
    "✓ Choose optimal data structures and algorithms\n",
    "✓ Ensure exact return format matches test expectations\n",
    "\n",
    "Generate the Python code solution that passes ALL test cases.\"\"\"\n",
    "        \n",
    "    # Format for chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "        \n",
    "    # Apply chat template\n",
    "    formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for inference\n",
    "instructions_data = []\n",
    "formatted_prompts = []\n",
    "ids_list = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), desc=\"Preparing data\", unit=\"row\", total=len(test_df)):\n",
    "    instructions_data.append({\n",
    "        'instruction': row['instruction'],\n",
    "        'test_list': row['test_list'],\n",
    "        'id': row['id']\n",
    "    })\n",
    "\n",
    "for item in tqdm(instructions_data, desc=\"Formatting prompts\", unit=\"prompt\"):\n",
    "    formatted_prompt = format_prompt(item)\n",
    "    formatted_prompts.append(formatted_prompt)\n",
    "    ids_list.append(item['id'])\n",
    "\n",
    "print(f\"Formatted {len(formatted_prompts)} prompts\")\n",
    "\n",
    "# Create dataset\n",
    "dataset_dict = {\n",
    "    'prompt': formatted_prompts,\n",
    "    'id': ids_list\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ac410",
   "metadata": {},
   "source": [
    "## Enhanced Generation with Test-Driven Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt):\n",
    "    try:\n",
    "        # Ensure proper device handling for tensors\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        result = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=768,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            return_full_text=False,\n",
    "            pad_token_id=pipe.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_code = result[0]['generated_text'].strip()\n",
    "        \n",
    "        return generated_code\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_code: {e}\")\n",
    "        return \"def placeholder(): pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e426f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Test-Driven Code Generation Loop with Fine-tuned Model\n",
    "responses = []\n",
    "failed_ids = []\n",
    "\n",
    "print(\"Starting inference with enhanced error handling...\")\n",
    "\n",
    "for idx in tqdm(range(len(dataset)), desc=\"Enhanced generation\"):\n",
    "    try:\n",
    "        # Get sample data\n",
    "        prompt = dataset[idx]['prompt']\n",
    "        sample_id = dataset[idx]['id']\n",
    "        \n",
    "        # Get corresponding test cases and instruction from original data\n",
    "        original_row = test_df[test_df['id'] == sample_id].iloc[0]\n",
    "        test_cases_str = original_row['test_list']\n",
    "        instruction = original_row['instruction']\n",
    "        \n",
    "        # Clear any potential GPU memory issues before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Call the corrected generate_code function\n",
    "        generated_code = generate_code(\n",
    "            prompt\n",
    "        )\n",
    "        \n",
    "        responses.append(generated_code)    \n",
    "        \n",
    "        # More frequent memory clearing to prevent accumulation issues\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            clear_memory()\n",
    "            print(f\"\\nMemory cleared after {idx + 1} samples\")\n",
    "            \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Progress Update after {idx + 1} samples:\")\n",
    "            print(f\"Successful: {len(responses) - len(failed_ids)}\")\n",
    "            print(f\"Failed: {len(failed_ids)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Complete failure for ID {sample_id}: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        failed_ids.append(sample_id)\n",
    "        responses.append(\"def placeholder(): pass\")\n",
    "        \n",
    "        # Clear memory after errors to prevent cascading issues\n",
    "        clear_memory()\n",
    "        \n",
    "        continue\n",
    "\n",
    "print(\"GENERATION WITH FINE-TUNED MODEL COMPLETED!\")\n",
    "print(f\"Final stats - Successful: {len(responses) - len(failed_ids)}, Failed: {len(failed_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save raw responses to binary file — NO ENCODING ISSUES!\n",
    "with open(\"raw_responses_backup.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)\n",
    "\n",
    "print(\"Successfully saved raw responses to 'raw_responses_backup.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json(\"test_df.json\", orient=\"records\", indent=2)\n",
    "print(\"Saved test_df to 'test_df.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
