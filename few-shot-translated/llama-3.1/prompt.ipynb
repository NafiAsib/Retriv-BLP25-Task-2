{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34f1fb75",
      "metadata": {},
      "source": [
        "## Setup and Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dac5dd55",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nafi/dev/shared-task/task2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import gc\n",
        "import ast\n",
        "import signal\n",
        "import sys\n",
        "from io import StringIO\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c42dda23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 3090 Ti\n",
            "GPU Memory: 22.0 GB\n"
          ]
        }
      ],
      "source": [
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected - using CPU (will be very slow)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b50c99",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "62c3a4ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_data_path = 'PATH_TO_TRANSLATED_DATESET'\n",
        "dev_df = pd.read_csv(dev_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fabc05ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "        \"text-generation\", \n",
        "        model=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "        trust_remote_code=False,  # Llama doesn't need trust_remote_code\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "627a493d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "        \"\"\"Format a single example into the required prompt format\"\"\"\n",
        "        instruction = example['instruction']\n",
        "        test_list = example['test_list']\n",
        "        \n",
        "        # Parse function name from instruction\n",
        "        function_name = \"unknown_function\"\n",
        "        if \"Example:\" in instruction:\n",
        "            example_part = instruction.split(\"Example:\")[1].strip()\n",
        "            func_match = re.search(r'(\\w+)\\s*\\(', example_part)\n",
        "            if func_match:\n",
        "                function_name = func_match.group(1)\n",
        "        \n",
        "       \n",
        "        system_message = \"\"\"You are an expert Python programmer. Your task is to generate clean, efficient, and correct Python functions that pass all given test cases.\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. ALWAYS wrap your code in ```python ``` blocks\n",
        "2. Write ONLY the function implementation, no extra explanations\n",
        "3. Use the EXACT function name from the example\n",
        "4. Ensure the function passes ALL test cases\n",
        "5. Handle edge cases and invalid inputs appropriately\n",
        "6. Use appropriate data types based on test case patterns\n",
        "\n",
        "Here are examples of how to solve different types of problems:\n",
        "\n",
        "EXAMPLE 1 - String Processing:\n",
        "Task: Write a Python function to find the first repeated character in a given string.\n",
        "Test Cases:\n",
        "assert first_repeated_char(\"abcabc\") == \"a\"\n",
        "assert first_repeated_char(\"abc\") == \"None\"  \n",
        "assert first_repeated_char(\"123123\") == \"1\"\n",
        "\n",
        "Expected Solution:\n",
        "```python\n",
        "def first_repeated_char(s):\n",
        "    seen = set()\n",
        "    for char in s:\n",
        "        if char in seen:\n",
        "            return char\n",
        "        seen.add(char)\n",
        "    return \"None\"\n",
        "```\n",
        "\n",
        "EXAMPLE 2 - Mathematical Function:\n",
        "Task: Write a function to check if a given integer is a prime number.\n",
        "Test Cases:\n",
        "assert prime_num(13) == True\n",
        "assert prime_num(7) == True\n",
        "assert prime_num(-1010) == False\n",
        "\n",
        "Expected Solution:\n",
        "```python\n",
        "def prime_num(n):\n",
        "    if n < 2:\n",
        "        return False\n",
        "    if n == 2:\n",
        "        return True\n",
        "    if n % 2 == 0:\n",
        "        return False\n",
        "    for i in range(3, int(n**0.5) + 1, 2):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "```\n",
        "\n",
        "\n",
        "Code Quality Standards:\n",
        "- Write code with proper indentation\n",
        "- Optimize for correctness first, then efficiency\n",
        "- Handle common edge cases (empty inputs, None values, negative numbers, etc.)\n",
        "- Return the exact data type shown in test cases\"\"\"\n",
        "        \n",
        "        user_prompt = f\"\"\"Generate a Python function for this problem:\n",
        "\n",
        "**Task**: {instruction}\n",
        "\n",
        "**Test Cases**:\n",
        "{test_list}\n",
        "\n",
        "**Expected Function Name**: {function_name}\n",
        "\n",
        "Requirements:\n",
        "- Follow the examples shown in the system message\n",
        "- Analyze the test cases carefully to understand input/output patterns\n",
        "- Implement the function to pass ALL test cases exactly\n",
        "- Return the appropriate data type as shown in test cases\n",
        "- Handle edge cases gracefully (empty inputs, invalid values, etc.)\n",
        "- Use efficient algorithms where applicable\n",
        "\n",
        "Generate ONLY the Python function wrapped in ```python ``` blocks. No explanations needed.\"\"\"\n",
        "        \n",
        "        # Format for Llama 3.1 using chat template\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "        \n",
        "        # Apply chat template\n",
        "        formatted_prompt = pipe.tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        return formatted_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50fa3c84",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing data: 100%|██████████| 400/400 [00:00<00:00, 67262.22row/s]\n",
            "Formatting prompts: 100%|██████████| 400/400 [00:00<00:00, 24291.57prompt/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted 400 prompts\n",
            "Creating dataset from formatted prompts...\n",
            "Dataset created with 400 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "instructions_data = []\n",
        "formatted_prompts = []\n",
        "ids_list = []\n",
        "for _, row in tqdm(dev_df.iterrows(), desc=\"Preparing data\", unit=\"row\", total=len(dev_df)):\n",
        "    instructions_data.append({\n",
        "        'instruction': row['instruction'],\n",
        "        'test_list': row['test_list'],\n",
        "        'id': row['id']\n",
        "    })\n",
        "\n",
        "for item in tqdm(instructions_data, desc=\"Formatting prompts\", unit=\"prompt\"):\n",
        "    formatted_prompt = format_prompt(item)\n",
        "    formatted_prompts.append(formatted_prompt)\n",
        "    ids_list.append(item['id'])\n",
        "\n",
        "print(f\"Formatted {len(formatted_prompts)} prompts\")\n",
        "print(\"Creating dataset from formatted prompts...\")\n",
        "\n",
        "dataset_dict = {\n",
        "    'prompt': formatted_prompts,\n",
        "    'id': ids_list\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "print(f\"Dataset created with {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "19d09fb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_function_name_from_test(test_cases_str):\n",
        "    \"\"\"Extract function name from test cases for better error reporting\"\"\"\n",
        "    try:\n",
        "        inner_str = ast.literal_eval(test_cases_str)\n",
        "        test_cases = ast.literal_eval(inner_str)\n",
        "        if test_cases:\n",
        "            # Find function name in first test case\n",
        "            func_match = re.search(r'assert\\s+(\\w+)\\s*\\(', test_cases[0])\n",
        "            if func_match:\n",
        "                return func_match.group(1)\n",
        "    except:\n",
        "        pass\n",
        "    return \"function\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b06945",
      "metadata": {},
      "source": [
        "## Simple Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f376c5f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_code(prompt):\n",
        "    \"\"\"\n",
        "    Generate code from the given prompt using the language model pipeline.\n",
        "    Returns: (generated_code)\n",
        "    \"\"\"\n",
        "    \n",
        "    result = pipe(\n",
        "                prompt,\n",
        "                max_new_tokens=768,  # Llama 3.1 works well with moderate length\n",
        "                temperature=0.1,     # Conservative temperature for code generation\n",
        "                top_p=0.95,         # Standard top_p for Llama\n",
        "                do_sample=True,\n",
        "                return_full_text=False,\n",
        "                pad_token_id=pipe.tokenizer.eos_token_id if hasattr(pipe.tokenizer, 'eos_token_id') else None\n",
        "            )\n",
        "            \n",
        "    generated_code = result[0]['generated_text'].strip()\n",
        "    \n",
        "    return generated_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "239e50fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:   2%|▎         | 10/400 [00:23<12:57,  1.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "🦙 Llama code generation:   5%|▌         | 20/400 [00:41<15:10,  2.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 20 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  10%|█         | 40/400 [01:11<10:37,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 40 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  12%|█▎        | 50/400 [01:28<09:51,  1.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Progress Update after 50 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  15%|█▌        | 60/400 [01:44<10:47,  1.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 60 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  20%|██        | 80/400 [02:14<07:12,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 80 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  25%|██▌       | 100/400 [02:53<11:10,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 100 samples\n",
            "\n",
            "📊 Progress Update after 100 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  30%|███       | 120/400 [03:24<08:09,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 120 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  35%|███▌      | 140/400 [04:04<07:52,  1.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 140 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  38%|███▊      | 150/400 [04:23<07:30,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Progress Update after 150 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  40%|████      | 160/400 [04:45<07:32,  1.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 160 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  45%|████▌     | 180/400 [05:25<07:29,  2.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 180 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  50%|█████     | 200/400 [06:03<04:31,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 200 samples\n",
            "\n",
            "📊 Progress Update after 200 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  55%|█████▌    | 220/400 [06:34<05:01,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 220 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  60%|██████    | 240/400 [07:16<04:46,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 240 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  62%|██████▎   | 250/400 [07:49<05:10,  2.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Progress Update after 250 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  65%|██████▌   | 260/400 [08:11<03:30,  1.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 260 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  70%|███████   | 280/400 [08:42<02:54,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 280 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  75%|███████▌  | 300/400 [09:12<02:10,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 300 samples\n",
            "\n",
            "📊 Progress Update after 300 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  80%|████████  | 320/400 [09:49<02:21,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 320 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  85%|████████▌ | 340/400 [10:28<01:31,  1.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 340 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  88%|████████▊ | 350/400 [10:45<01:40,  2.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Progress Update after 350 samples:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  90%|█████████ | 360/400 [11:02<01:05,  1.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 360 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation:  95%|█████████▌| 380/400 [11:36<00:38,  1.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 380 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🦙 Llama code generation: 100%|██████████| 400/400 [12:16<00:00,  1.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Memory cleared after 400 samples\n",
            "\n",
            "📊 Progress Update after 400 samples:\n",
            "LLAMA CODE GENERATION COMPLETED!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "\n",
        "for idx in tqdm(range(len(dataset)), desc=\"🦙 Llama code generation\"):\n",
        "    try:\n",
        "        # Get sample data\n",
        "        prompt = dataset[idx]['prompt']\n",
        "        sample_id = dataset[idx]['id']\n",
        "        \n",
        "        # Get corresponding test cases and instruction from original data\n",
        "        original_row = dev_df[dev_df['id'] == sample_id].iloc[0]\n",
        "        test_cases_str = original_row['test_list']\n",
        "        instruction = original_row['instruction']\n",
        "        \n",
        "        # Generate code\n",
        "        generated_code = generate_code(prompt)\n",
        "        \n",
        "        responses.append(generated_code)   \n",
        "        \n",
        "        # Memory management - clear every 20 samples\n",
        "        if (idx + 1) % 20 == 0:\n",
        "            clear_memory()\n",
        "            print(f\"\\n🧠 Memory cleared after {idx + 1} samples\")\n",
        "            \n",
        "        # Progress update every 50 samples\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"\\n📊 Progress Update after {idx + 1} samples:\") \n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Complete failure for ID {sample_id}: {e}\")\n",
        "        responses.append(\"def placeholder(): pass\")\n",
        "        continue\n",
        "\n",
        "\n",
        "print(\"LLAMA CODE GENERATION COMPLETED!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d5b530b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUBMISSION SAVED!\n",
            "File: submission.json\n",
            "Total samples: 400\n"
          ]
        }
      ],
      "source": [
        "submission_data = []\n",
        "for i, (_, row) in enumerate(dev_df.iterrows()):\n",
        "    submission_data.append({\n",
        "        \"id\": int(row['id']),\n",
        "        \"response\": responses[i]\n",
        "    })\n",
        "\n",
        "submission_file = \"submission.json\"\n",
        "with open(submission_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(submission_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "print(\"SUBMISSION SAVED!\")\n",
        "print(f\"File: {submission_file}\")\n",
        "print(f\"Total samples: {len(submission_data)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
